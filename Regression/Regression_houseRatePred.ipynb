{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated valued:\n",
      "449980.0\n"
     ]
    }
   ],
   "source": [
    "def estimate_home_value(size_in_sqft, number_of_bedrooms):\n",
    "\n",
    "    # Assume all homes are worth at least $50,000\n",
    "    value = 50000\n",
    "\n",
    "    # Adjust the value estimate based on the size of the house\n",
    "    value = value + (size_in_sqft * 92.1)\n",
    "\n",
    "    # Adjust the value estimate based on the number of bedrooms\n",
    "    value = value + (number_of_bedrooms * 10000)\n",
    "\n",
    "    return value\n",
    "\n",
    "# Estimate the value of our house:\n",
    "# - 5 bedrooms\n",
    "# - 3800 sq ft\n",
    "# Actual value: $450,000\n",
    "\n",
    "value = estimate_home_value(3800, 5)\n",
    "\n",
    "print(\"Estimated valued:\")\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import webbrowser\n",
    "import os\n",
    "\n",
    "# Read the dataset into a data table using Pandas\n",
    "data_table = pandas.read_csv(\"ml_house_data_set.csv\")\n",
    "\n",
    "# Create a web page view of the data for easy viewing\n",
    "html = data_table[0:100].to_html()\n",
    "\n",
    "# Save the html to a temporary file\n",
    "with open(\"data.html\", \"w\") as f:\n",
    "    f.write(html)\n",
    "\n",
    "# Open the web page in our web browser\n",
    "full_filename = os.path.abspath(\"data.html\")\n",
    "webbrowser.open(\"file://{}\".format(full_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1978L, 1L, 4L, ..., 0, 0, 0],\n",
       "        [1958L, 1L, 3L, ..., 0, 0, 0],\n",
       "        [2002L, 1L, 3L, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [1983L, 1L, 1L, ..., 0, 0, 0],\n",
       "        [1981L, 1L, 3L, ..., 0, 0, 0],\n",
       "        [1980L, 1L, 3L, ..., 0, 0, 0]], dtype=object),\n",
       " array([  270897.,   302404.,  2519996., ...,    98280.,    98278.,\n",
       "          186480.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Varies with sklearn version\n",
    "from sklearn.cross_validation import train_test_split\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Load the data set\n",
    "df = pd.read_csv(\"ml_house_data_set.csv\")\n",
    "\n",
    "# Remove the fields from the data set that we don't want to include in our model\n",
    "del df['house_number']\n",
    "del df['unit_number']\n",
    "del df['street_name']\n",
    "del df['zip_code']\n",
    "\n",
    "# Replace categorical data with one-hot encoded data\n",
    "features_df = pd.get_dummies(df, columns=['garage_type', 'city'])\n",
    "\n",
    "# Remove the sale price from the feature data\n",
    "del features_df['sale_price']\n",
    "\n",
    "# Create the X and y arrays\n",
    "X = features_df.as_matrix()\n",
    "y = df['sale_price'].as_matrix()\n",
    "\n",
    "X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_df.to_csv('SampleX.csv')\n",
    "df['sale_price'].to_csv('SampleY.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2013L, 2L, 5L, ..., 0, 0, 0],\n",
       "        [1979L, 1L, 3L, ..., 0, 0, 0],\n",
       "        [1959L, 1L, 3L, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [2007L, 2L, 5L, ..., 0, 0, 0],\n",
       "        [1967L, 1L, 2L, ..., 0, 0, 0],\n",
       "        [2005L, 1L, 2L, ..., 0, 0, 0]], dtype=object),\n",
       " array([[1990L, 1L, 4L, ..., 0, 0, 0],\n",
       "        [2006L, 1L, 2L, ..., 0, 0, 0],\n",
       "        [1973L, 1L, 2L, ..., 0, 1, 0],\n",
       "        ..., \n",
       "        [1973L, 1L, 2L, ..., 0, 0, 0],\n",
       "        [1987L, 2L, 3L, ..., 0, 0, 0],\n",
       "        [1976L, 1L, 4L, ..., 0, 0, 0]], dtype=object),\n",
       " array([ 538023.,  932398.,  214198., ...,  338938.,  378004.,  304924.]),\n",
       " array([ 264598.,  338943.,  146159., ...,  173876.,  463676.,   88198.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data set in a training set (70%) and a test set (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='huber', max_depth=6,\n",
       "             max_features=0.1, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=9,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=1000, presort='auto', random_state=0,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit regression model\n",
    "model = ensemble.GradientBoostingRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=9,\n",
    "    max_features=0.1,\n",
    "    loss='huber',\n",
    "    random_state=0\n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/trained_house_classifier_model.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model to a file so we can use it in other programs. Create a folder named model in the directory.\n",
    "joblib.dump(model, 'model/trained_house_classifier_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Mean Absolute Error: 48187.1991\n",
      "Test Set Mean Absolute Error: 59174.2563\n"
     ]
    }
   ],
   "source": [
    "# Find the error rate on the training set\n",
    "mae = mean_absolute_error(y_train, model.predict(X_train))\n",
    "print(\"Training Set Mean Absolute Error: %.4f\" % mae)\n",
    "\n",
    "# Find the error rate on the test set\n",
    "mae = mean_absolute_error(y_test, model.predict(X_test))\n",
    "print(\"Test Set Mean Absolute Error: %.4f\" % mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city_Julieberg - 0.00%\n",
      "city_Martinezfort - 0.00%\n",
      "city_New Michele - 0.00%\n",
      "city_New Robinton - 0.00%\n",
      "city_Davidtown - 0.07%\n",
      "city_West Brittanyview - 0.08%\n",
      "city_Rickytown - 0.09%\n",
      "city_West Terrence - 0.10%\n",
      "city_Port Daniel - 0.11%\n",
      "city_Amystad - 0.12%\n",
      "city_Fosterberg - 0.12%\n",
      "city_East Justin - 0.12%\n",
      "city_Leahview - 0.14%\n",
      "city_Lake Jennifer - 0.14%\n",
      "city_Toddshire - 0.14%\n",
      "city_Clarkberg - 0.18%\n",
      "city_Jenniferberg - 0.18%\n",
      "city_West Lydia - 0.19%\n",
      "city_Wendybury - 0.19%\n",
      "city_Lake Carolyn - 0.20%\n",
      "city_West Gerald - 0.20%\n",
      "city_Joshuafurt - 0.20%\n",
      "city_Brownport - 0.21%\n",
      "city_East Lucas - 0.22%\n",
      "city_Davidfort - 0.27%\n",
      "city_South Stevenfurt - 0.27%\n",
      "city_Port Jonathanborough - 0.27%\n",
      "city_Port Adamtown - 0.29%\n",
      "city_Scottberg - 0.29%\n",
      "city_East Janiceville - 0.31%\n",
      "city_Justinport - 0.34%\n",
      "city_East Amychester - 0.34%\n",
      "city_Lake Dariusborough - 0.36%\n",
      "city_West Gregoryview - 0.37%\n",
      "city_Richardport - 0.37%\n",
      "city_Lake Christinaport - 0.37%\n",
      "city_Morrisport - 0.39%\n",
      "city_Hallfort - 0.41%\n",
      "city_North Erinville - 0.56%\n",
      "city_Jeffreyhaven - 0.60%\n",
      "city_South Anthony - 0.61%\n",
      "city_West Ann - 0.72%\n",
      "city_Port Andrealand - 0.73%\n",
      "has_central_heating - 0.80%\n",
      "city_Chadstad - 0.83%\n",
      "city_Lewishaven - 0.89%\n",
      "garage_type_detached - 0.90%\n",
      "city_Coletown - 0.91%\n",
      "has_central_cooling - 0.92%\n",
      "city_Lake Jack - 1.01%\n",
      "garage_type_attached - 1.09%\n",
      "garage_type_none - 1.12%\n",
      "has_pool - 1.84%\n",
      "half_bathrooms - 1.88%\n",
      "has_fireplace - 1.96%\n",
      "stories - 2.19%\n",
      "carport_sqft - 3.86%\n",
      "full_bathrooms - 3.89%\n",
      "num_bedrooms - 4.92%\n",
      "year_built - 12.94%\n",
      "garage_sqft - 13.40%\n",
      "livable_sqft - 16.41%\n",
      "total_sqft - 17.33%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# These are the feature labels from our data set\n",
    "feature_labels = np.array(['year_built', 'stories', 'num_bedrooms', 'full_bathrooms', 'half_bathrooms', 'livable_sqft', 'total_sqft', 'garage_sqft', 'carport_sqft', 'has_fireplace', 'has_pool', 'has_central_heating', 'has_central_cooling', 'garage_type_attached', 'garage_type_detached', 'garage_type_none', 'city_Amystad', 'city_Brownport', 'city_Chadstad', 'city_Clarkberg', 'city_Coletown', 'city_Davidfort', 'city_Davidtown', 'city_East Amychester', 'city_East Janiceville', 'city_East Justin', 'city_East Lucas', 'city_Fosterberg', 'city_Hallfort', 'city_Jeffreyhaven', 'city_Jenniferberg', 'city_Joshuafurt', 'city_Julieberg', 'city_Justinport', 'city_Lake Carolyn', 'city_Lake Christinaport', 'city_Lake Dariusborough', 'city_Lake Jack', 'city_Lake Jennifer', 'city_Leahview', 'city_Lewishaven', 'city_Martinezfort', 'city_Morrisport', 'city_New Michele', 'city_New Robinton', 'city_North Erinville', 'city_Port Adamtown', 'city_Port Andrealand', 'city_Port Daniel', 'city_Port Jonathanborough', 'city_Richardport', 'city_Rickytown', 'city_Scottberg', 'city_South Anthony', 'city_South Stevenfurt', 'city_Toddshire', 'city_Wendybury', 'city_West Ann', 'city_West Brittanyview', 'city_West Gerald', 'city_West Gregoryview', 'city_West Lydia', 'city_West Terrence'])\n",
    "\n",
    "# Load the trained model created with train_model.py\n",
    "model = joblib.load('model/trained_house_classifier_model.pkl')\n",
    "\n",
    "# Create a numpy array based on the model's feature importances\n",
    "importance = model.feature_importances_\n",
    "\n",
    "# Sort the feature labels based on the feature importance rankings from the model\n",
    "feauture_indexes_by_importance = importance.argsort()\n",
    "\n",
    "# Print each feature label, from most important to least important (reverse order)\n",
    "for index in feauture_indexes_by_importance:\n",
    "    print(\"{} - {:.2f}%\".format(feature_labels[index], (importance[index] * 100.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Mean Absolute Error: 48779.4809\n",
      "Test Set Mean Absolute Error: 58957.8640\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Load the data set\n",
    "df = pd.read_csv(\"ml_house_data_set.csv\")\n",
    "\n",
    "# Remove the fields from the data set that we don't want to include in our model\n",
    "del df['house_number']\n",
    "del df['unit_number']\n",
    "del df['street_name']\n",
    "del df['zip_code']\n",
    "\n",
    "# Replace categorical data with one-hot encoded data\n",
    "features_df = pd.get_dummies(df, columns=['garage_type', 'city'])\n",
    "\n",
    "# Remove the sale price from the feature data\n",
    "del features_df['sale_price']\n",
    "\n",
    "# Create the X and y arrays\n",
    "X = features_df.as_matrix()\n",
    "y = df['sale_price'].as_matrix()\n",
    "\n",
    "# Split the data set in a training set (70%) and a test set (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit regression model\n",
    "model = ensemble.GradientBoostingRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=9,\n",
    "    max_features=0.1,\n",
    "    loss='huber'\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model to a file so we can use it in other programs\n",
    "joblib.dump(model, 'model/trained_house_classifier_model.pkl')\n",
    "\n",
    "# Find the error rate on the training set\n",
    "mse = mean_absolute_error(y_train, model.predict(X_train))\n",
    "print(\"Training Set Mean Absolute Error: %.4f\" % mse)\n",
    "\n",
    "# Find the error rate on the test set\n",
    "mse = mean_absolute_error(y_test, model.predict(X_test))\n",
    "print(\"Test Set Mean Absolute Error: %.4f\" % mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Load the data set\n",
    "df = pd.read_csv(\"ml_house_data_set.csv\")\n",
    "\n",
    "# Remove the fields from the data set that we don't want to include in our model\n",
    "del df['house_number']\n",
    "del df['unit_number']\n",
    "del df['street_name']\n",
    "del df['zip_code']\n",
    "\n",
    "# Replace categorical data with one-hot encoded data\n",
    "features_df = pd.get_dummies(df, columns=['garage_type', 'city'])\n",
    "del features_df['sale_price']\n",
    "\n",
    "X = features_df.as_matrix()\n",
    "y = df['sale_price'].as_matrix()\n",
    "\n",
    "# Split the data set in a training set (70%) and a test set (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (DT) : 111283.3403\n",
      "Mean Absolute Error (AdaBoost) : 60062.8151\n"
     ]
    }
   ],
   "source": [
    "# Fit regression model\n",
    "regr_1 = DecisionTreeRegressor(max_depth=4)\n",
    "\n",
    "regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=50),n_estimators=70)\n",
    "\n",
    "regr_1.fit(X_train, y_train)\n",
    "regr_2.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_1 = regr_1.predict(X_test)\n",
    "y_2 = regr_2.predict(X_test)\n",
    "\n",
    "mse = mean_absolute_error(y_test, y_1)\n",
    "print(\"Mean Absolute Error (DT) : %.4f\" % mse)\n",
    "\n",
    "mse = mean_absolute_error(y_test, y_2)\n",
    "print(\"Mean Absolute Error (AdaBoost) : %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 96230.1727\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit (X_train,y_train)\n",
    "pred=reg.predict(X_test)\n",
    "mse = mean_absolute_error(y_test,pred)\n",
    "print(\"Mean Absolute Error : %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 1230961360496583.2500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "reg = linear_model.SGDRegressor()\n",
    "reg.fit (X_train,y_train)\n",
    "pred=reg.predict(X_test)\n",
    "mse = mean_absolute_error(y_test,pred)\n",
    "print(\"Mean Absolute Error : %.4f\" % mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Mean Absolute Error: 1232390011428614.5000\n",
      "Test Set Mean Absolute Error: 1230961360496583.2500\n"
     ]
    }
   ],
   "source": [
    "#Let's find if it underfits...\n",
    "# Find the error rate on the training set\n",
    "mse = mean_absolute_error(y_train, reg.predict(X_train))\n",
    "print(\"Training Set Mean Absolute Error: %.4f\" % mse)\n",
    "\n",
    "# Find the error rate on the test set\n",
    "mse = mean_absolute_error(y_test, reg.predict(X_test))\n",
    "print(\"Test Set Mean Absolute Error: %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 64905.4873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "reg = RandomForestRegressor()\n",
    "reg.fit (X_train,y_train)\n",
    "pred=reg.predict(X_test)\n",
    "mse = mean_absolute_error(y_test,pred)\n",
    "print(\"Mean Absolute Error : %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 96231.9639\n"
     ]
    }
   ],
   "source": [
    "## Naive Bayes\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "reg = BayesianRidge()\n",
    "reg.fit (X_train,y_train)\n",
    "pred=reg.predict(X_test)\n",
    "mse = mean_absolute_error(y_test,pred)\n",
    "print(\"Mean Absolute Error : %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 171395.1292\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "reg = SVR()\n",
    "reg.fit (X_train,y_train)\n",
    "pred=reg.predict(X_test)\n",
    "mse = mean_absolute_error(y_test,pred)\n",
    "print(\"Mean Absolute Error : %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Load the data set\n",
    "df = pandas.read_csv(\"ml_house_data_set.csv\")\n",
    "\n",
    "# Remove the fields from the data set that we don't want to include in our model\n",
    "del df['house_number']\n",
    "del df['unit_number']\n",
    "del df['street_name']\n",
    "del df['zip_code']\n",
    "\n",
    "# Replace categorical data with one-hot encoded data\n",
    "features_df = pandas.get_dummies(df, columns=['garage_type', 'city'])\n",
    "del features_df['sale_price']\n",
    "\n",
    "X = features_df.as_matrix()\n",
    "y = df['sale_price'].as_matrix()\n",
    "\n",
    "# Split the data set in a training set (70%) and a test set (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Create the model\n",
    "model = ensemble.GradientBoostingRegressor()\n",
    "\n",
    "# Parameters we want to try\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 3000],\n",
    "    'max_depth': [4, 6],\n",
    "    'min_samples_leaf': [3, 9],\n",
    "    'learning_rate': [0.1, 0.05],\n",
    "    'max_features': [0.3, 0.1],\n",
    "    'loss': ['ls', 'huber']\n",
    "}\n",
    "\n",
    "#param_grid = {\n",
    "#    'n_estimators': [500, 1000, 3000],\n",
    "#    'max_depth': [4, 6],\n",
    "#   'min_samples_leaf': [3, 5, 9, 17],\n",
    "#    'learning_rate': [0.1, 0.05, 0.02, 0.01],\n",
    "#    'max_features': [1.0, 0.3, 0.1],\n",
    "#    'loss': ['ls', 'lad', 'huber']\n",
    "#}\n",
    "\n",
    "# Define the grid search we want to run. Run it with four cpus in parallel. -1 all cores\n",
    "gs_cv = GridSearchCV(model, param_grid, n_jobs=-1)\n",
    "\n",
    "# Run the grid search - on only the training data!\n",
    "gs_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the parameters that gave us the best result!\n",
    "print(gs_cv.best_params_)\n",
    "\n",
    "# After running a .....long..... time, the output will be something like\n",
    "# {'loss': 'huber', 'learning_rate': 0.1, 'min_samples_leaf': 9, 'n_estimators': 3000, 'max_features': 0.1, 'max_depth': 6}\n",
    "\n",
    "# That is the combination that worked best.\n",
    "\n",
    "# Find the error rate on the training set using the best parameters\n",
    "mse = mean_absolute_error(y_train, gs_cv.predict(X_train))\n",
    "print(\"Training Set Mean Absolute Error: %.4f\" % mse)\n",
    "\n",
    "# Find the error rate on the test set using the best parameters\n",
    "mse = mean_absolute_error(y_test, gs_cv.predict(X_test))\n",
    "print(\"Test Set Mean Absolute Error: %.4f\" % mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Datasets\n",
    "Auto MPG : https://archive.ics.uci.edu/ml/datasets/auto+mpg\n",
    "    \n",
    "Auto MPG CSV format : https://www.kaggle.com/uciml/autompg-dataset    \n",
    "        \n",
    "Bike Sharing : https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
    "        \n",
    "Actual Feed: https://www.motivateco.com/use-our-data/\n",
    "        \n",
    "CSV Format: https://www.kaggle.com/marklvl/bike-sharing-dataset"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
